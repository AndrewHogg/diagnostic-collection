This directory contains a set of scripts to generate a diagnostic tarball for DSE or DDAC
installations, similar (and partially compatible) to diagnostic tarball generated by
OpsCenter. (scripts were tested on MacOS and on Ubuntu Linux).

Generation of diagnostic tarball for DSE/DDAC consists of 2 parts:
1. Collection of data on individual nodes
2. Merging all data into single file

These 2 steps are separate because admins are using different ways to access nodes in the
cluster, and transferring the data.  We're providing the sample script that allows to
perform the data collection from cluster nodes - see description of the `collect_diag.sh`
script.

## Collecting diagnostic on individual nodes

Collection of the data on individual nodes is performed by `collect_node_diag.sh` script
that performs following actions:
* Collects all configuration files;
* Collect all log files;
* Execute a number of `nodetool` sub-commands, like, `tablestats`, `tpstats`, etc.;
* Execute `dsetool status` and `dsetool ring` commands;
* Collects database schema;
* Execute a number of commands to collect performance information - `iostat`, `vmstat`, etc.
* Collects additional data about system configuration - from `sysctl`, `/proc`, etc.

Script should be executed on every node of cluster, and if it's a tarball installation,
you need to pass one required parameter - full path to the root directory of DSE
installation. For package installation, location of the files will be detected
automatically, without specification of the root directory.  There are also optional
parameters, that could be provided if, for example, you have authentication enabled for
Cassandra or JMX, changed JMX port, etc. (pass `-h` to get list of options):

* `-n` - specifies a list of general options to pass to `nodetool` (JMX user, password, etc.);
* `-c` - specifies options to pass to `cqlsh` (user name, password, etc.);
* `-d` - specifies the options for `dsetool` command (JMX user, password, etc.);
* `-p` - specifies the PID of DSE process.  Script tries to detect it from the output of
  `ps`, but this may not work reliably in case if you have several Cassandra processes on
  the same machine.  This PID is used to get information about limits set for process, etc.;
* `-f` - specifies the name where it should put the collected results (could be useful for
  some automation);
* `-i` - specifies that we need to collect data for DataStax Insights.
* `-t` - specifies the type of installation: `dse`, `ddac`, `coss` (default: `dse`);
* `-v` - enables more verbose output by all scripts

After successful execution, script generates file with name
`/var/tmp/dse-diag-<IP_Address>.tar.gz`, like, `/var/tmp/dse-diag-10.200.179.237.tar.gz`,
or into specified by option `-f` (or if the `-w` flag was used, the file name prefix will
be `ddac-diag-...`).

## Merging all diagnostics into single tarball

After the `collect_node_diag.sh` script was executed on every machine of the cluster,
generated files should be collected into single directory on one machine to be merged
using the `generate_diag.sh` script.  This script accepts single parameter - path to
directory with collected files (it could be either relative, or absolute).  There are also
optional parameters:

* `-f` - specifies path to file where data should be put;
* `-i` - specifies that we need to merge Insights data;
* `-p` - specifies the custom pattern for file names if `collect_node_diag.sh` was called
  with `-f` parameter - otherwise this script may not find the data.
* `-t` - specifies the type of installation: `dse`, `ddac`, `coss` (default: `dse`).

This script performs following:

* Creates a temporary directory in the `/var/tmp`;
* Unpacks each of collected files;
* Removes sensitive information, such as, passwords from configuration files;
* Packs everything together into single file that has name
  `<cluster_name>-diagnostics.tar.gz`, for example, `dsetest-diagnostics.tar.gz`, or into
  specified by `-f` option.

This file could be then sent to DataStax support for analysis.

## collect_diag.sh

This script is performing copying of the `collect_node_diag.sh` file into nodes, executing
it, copying back collected data, and generate resulting tarball with DSE Insights or
diagnostic data.

```
Usage: ./collect_diag.sh -t <type> [options] [path]
 ----- Required --------
   -t type -  valid choices are "coss", "ddac", "dse"
 ----- Options --------
   -c cqlsh_options - e.g "-u user -p password" etc. Ensure you enclose with "
   -d dsetool_options - options to pass to dsetool. Syntax the same as "-c"
   -f file_name - file with list of hosts where to execute command (default - try to get list from 'nodetool status')
   -i insights - collect only data for DSE Insights
   -I insights_dir - directory that contains insights .gz files
   -n nodetool_options - options to pass to nodetool. Syntax the same as "-c"
   -o output_dir - where to put resulting file (default: /var/folders/js/stc2gc756bs2s2d2_kx34svw0000gn/T/tmp.iEP7he6g)
   -p pid - PID of DSE or DDAC process
   -r - remove collected files after generation of resulting tarball
   -s ssh/pssh/scp options - options to pass to SSH/PSSH/SCP
   -u timeout - timeout for PSSH/SSH in seconds (default: 600)
   -v - verbose output
   path - top directory of COSS, DDAC or DSE installation (for tarball installs)
```

Please note that user name should be passed as `-o User=...` in `-s` option, as `scp` and
`ssh` are using different ways to pass user name.

Example of usage with list of hosts passed explicitly (file `mhosts`):

```sh
./collect_diag.sh -t dse -f mhosts -r -s \
  "-i ~/.ssh/private_key -o StrictHostKeyChecking=no -o User=automaton"
```

Or for DDAC:

```sh
./collect_diag.sh -t ddac -f mhosts -r -s \
  "-i ~/.ssh/private_key -o StrictHostKeyChecking=no -o User=automaton" \
  /usr/local/lib/cassandra
```

if it's running on the machine with DSE/DDAC/C*, then it could be as simple as, as it will
use `nodetool status` to obtain a list of nodes:

```sh
./collect_diag.sh -t ddac -r /usr/local/lib/cassandra
```
